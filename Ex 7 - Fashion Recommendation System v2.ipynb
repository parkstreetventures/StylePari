{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing these “magic commands” will inform your notebook to actively reload all imported modules and packages \n",
    "# as they are modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "import string\n",
    "import ast\n",
    "import re\n",
    "import unidecode\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "import pickle \n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import unidecode, ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to all files\n",
    "DATA_PATH = \"./data/saree_data.csv\"\n",
    "PARSED_PATH = \"./data/saree_parsed.csv\"\n",
    "TFIDF_ENCODING_PATH = \"./model/saree_tfidf_encodings.pkl\"\n",
    "TFIDF_MODEL_PATH = \"./model/saree_tfidf.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weigths and measures are words that will not add value to the model. I got these standard words from \n",
    "# https://en.wikibooks.org/wiki/Cookbook:Units_of_measurement\n",
    "\n",
    "# # We lemmatize the words to reduce them to their smallest form (lemmas). \n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# measures = [lemmatizer.lemmatize(m) for m in measures]\n",
    "# words_to_remove = [lemmatizer.lemmatize(m) for m in words_to_remove]\n",
    "\n",
    "def ingredient_parser(ingreds):\n",
    "    '''\n",
    "    \n",
    "    This function takes in a list (but it is a string as it comes from pandas dataframe) of \n",
    "       ingredients and performs some preprocessing. \n",
    "       For example:\n",
    "       input = '['1 x 1.6kg whole duck', '2 heaped teaspoons Chinese five-spice powder', '1 clementine',\n",
    "                 '6 fresh bay leaves', 'GRAVY', '', '1 bulb of garlic', '2 carrots', '2 red onions', \n",
    "                 '3 tablespoons plain flour', '100 ml Marsala', '1 litre organic chicken stock']'\n",
    "       \n",
    "       output = ['duck', 'chinese five spice powder', 'clementine', 'fresh bay leaf', 'gravy', 'garlic',\n",
    "                 'carrot', 'red onion', 'plain flour', 'marsala', 'organic chicken stock']\n",
    "    '''\n",
    "    measures = ['teaspoon', 't', 'tsp.', 'tablespoon', 'T', 'tbl.', 'tb', 'tbsp.', 'fluid ounce', 'fl oz', 'gill', 'cup', 'c', 'pint', 'p', 'pt', 'fl pt', 'quart', 'q', 'qt', 'fl qt', 'gallon', 'g', 'gal', 'ml', 'milliliter', 'millilitre', 'cc', 'mL', 'l', 'liter', 'litre', 'L', 'dl', 'deciliter', 'decilitre', 'dL', 'bulb', 'level', 'heaped', 'rounded', 'whole', 'pinch', 'medium', 'slice', 'pound', 'lb', '#', 'ounce', 'oz', 'mg', 'milligram', 'milligramme', 'g', 'gram', 'gramme', 'kg', 'kilogram', 'kilogramme', 'x', 'of', 'mm', 'millimetre', 'millimeter', 'cm', 'centimeter', 'centimetre', 'm', 'meter', 'metre', 'inch', 'in', 'milli', 'centi', 'deci', 'hecto', 'kilo']\n",
    "    words_to_remove = ['(',')','.','\\'','fresh', 'oil', 'a', 'red', 'bunch', 'and', 'clove', 'or', 'leaf', 'chilli', 'large', 'extra', 'sprig', 'ground', 'handful', 'free', 'small', 'pepper', 'virgin', 'range', 'from', 'dried', 'sustainable', 'black', 'peeled', 'higher', 'welfare', 'seed', 'for', 'finely', 'freshly', 'sea', 'quality', 'white', 'ripe', 'few', 'piece', 'source', 'to', 'organic', 'flat', 'smoked', 'ginger', 'sliced', 'green', 'picked', 'the', 'stick', 'plain', 'plus', 'mixed', 'mint', 'bay', 'basil', 'your', 'cumin', 'optional', 'fennel', 'serve', 'mustard', 'unsalted', 'baby', 'paprika', 'fat', 'ask', 'natural', 'skin', 'roughly', 'into', 'such', 'cut', 'good', 'brown', 'grated', 'trimmed', 'oregano', 'powder', 'yellow', 'dusting', 'knob', 'frozen', 'on', 'deseeded', 'low', 'runny', 'balsamic', 'cooked', 'streaky', 'nutmeg', 'sage', 'rasher', 'zest', 'pin', 'groundnut', 'breadcrumb', 'turmeric', 'halved', 'grating', 'stalk', 'light', 'tinned', 'dry', 'soft', 'rocket', 'bone', 'colour', 'washed', 'skinless', 'leftover', 'splash', 'removed', 'dijon', 'thick', 'big', 'hot', 'drained', 'sized', 'chestnut', 'watercress', 'fishmonger', 'english', 'dill', 'caper', 'raw', 'worcestershire', 'flake', 'cider', 'cayenne', 'tbsp', 'leg', 'pine', 'wild', 'if', 'fine', 'herb', 'almond', 'shoulder', 'cube', 'dressing', 'with', 'chunk', 'spice', 'thumb', 'garam', 'new', 'little', 'punnet', 'peppercorn', 'shelled', 'saffron', 'other''chopped', 'salt', 'olive', 'taste', 'can', 'sauce', 'water', 'diced', 'package', 'italian', 'shredded', 'divided', 'parsley', 'vinegar', 'all', 'purpose', 'crushed', 'juice', 'more', 'coriander', 'bell', 'needed', 'thinly', 'boneless', 'half', 'thyme', 'cubed', 'cinnamon', 'cilantro', 'jar', 'seasoning', 'rosemary', 'extract', 'sweet', 'baking', 'beaten', 'heavy', 'seeded', 'tin', 'vanilla', 'uncooked', 'crumb', 'style', 'thin', 'nut', 'coarsely', 'spring', 'chili', 'cornstarch', 'strip', 'cardamom', 'rinsed', 'honey', 'cherry', 'root', 'quartered', 'head', 'softened', 'container', 'crumbled', 'frying', 'lean', 'cooking', 'roasted', 'warm', 'whipping', 'thawed', 'corn', 'pitted', 'sun', 'kosher', 'bite', 'toasted', 'lasagna', 'split', 'melted', 'degree', 'lengthwise', 'romano', 'packed', 'pod', 'anchovy', 'rom', 'prepared', 'juiced', 'fluid', 'floret', 'room', 'active', 'seasoned', 'mix', 'deveined', 'lightly', 'anise', 'thai', 'size', 'unsweetened', 'torn', 'wedge', 'sour', 'basmati', 'marinara', 'dark', 'temperature', 'garnish', 'bouillon', 'loaf', 'shell', 'reggiano', 'canola', 'parmigiano', 'round', 'canned', 'ghee', 'crust', 'long', 'broken', 'ketchup', 'bulk', 'cleaned', 'condensed', 'sherry', 'provolone', 'cold', 'soda', 'cottage', 'spray', 'tamarind', 'pecorino', 'shortening', 'part', 'bottle', 'sodium', 'cocoa', 'grain', 'french', 'roast', 'stem', 'link', 'firm', 'asafoetida', 'mild', 'dash', 'boiling']\n",
    "    # The ingredient list is now a string so we need to turn it back into a list. We use ast.literal_eval\n",
    "    if isinstance(ingreds, list):\n",
    "        ingredients = ingreds\n",
    "    else:\n",
    "        ingredients = ast.literal_eval(ingreds)\n",
    "    # We first get rid of all the punctuation. We make use of str.maketrans. It takes three input \n",
    "    # arguments 'x', 'y', 'z'. 'x' and 'y' must be equal-length strings and characters in 'x'\n",
    "    # are replaced by characters in 'y'. 'z' is a string (string.punctuation here) where each character\n",
    "    #  in the string is mapped to None. \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ingred_list = []\n",
    "    for i in ingredients:\n",
    "        i.translate(translator)\n",
    "        # We split up with hyphens as well as spaces\n",
    "        items = re.split(' |-', i)\n",
    "        # Get rid of words containing non alphabet letters\n",
    "        items = [word for word in items if word.isalpha()]\n",
    "        # Turn everything to lowercase\n",
    "        items = [word.lower() for word in items]\n",
    "        # remove accents\n",
    "        items = [unidecode.unidecode(word) for word in items] #''.join((c for c in unicodedata.normalize('NFD', items) if unicodedata.category(c) != 'Mn'))\n",
    "        # Lemmatize words so we can compare words to measuring words\n",
    "        items = [lemmatizer.lemmatize(word) for word in items]\n",
    "        # Gets rid of measuring words/phrases, e.g. heaped teaspoon\n",
    "        items = [word for word in items if word not in measures]\n",
    "        # Get rid of common easy words\n",
    "        items = [word for word in items if word not in words_to_remove]\n",
    "        if items:\n",
    "            ingred_list.append(' '.join(items)) \n",
    "    ingred_list = \" \".join(ingred_list)\n",
    "    return ingred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecSys(ingredients, N=5):\n",
    "    \"\"\"\n",
    "    The reccomendation system takes in a list of ingredients and returns a list of top 5 \n",
    "    recipes based of of cosine similarity. \n",
    "    :param ingredients: a list of ingredients\n",
    "    :param N: the number of reccomendations returned \n",
    "    :return: top 5 reccomendations for cooking recipes\n",
    "    \"\"\"\n",
    "\n",
    "    # load in tdidf model and encodings \n",
    "    with open(TFIDF_ENCODING_PATH, 'rb') as f:\n",
    "        tfidf_encodings = pickle.load(f)\n",
    "\n",
    "    with open(TFIDF_MODEL_PATH, \"rb\") as f:\n",
    "        tfidf = pickle.load(f)\n",
    "\n",
    "    # parse the ingredients using my ingredient_parser \n",
    "    try: \n",
    "        ingredients_parsed = ingredient_parser(ingredients)\n",
    "    except:\n",
    "        ingredients_parsed = ingredient_parser([ingredients])\n",
    "    \n",
    "    # use our pretrained tfidf model to encode our input ingredients\n",
    "    ingredients_tfidf = tfidf.transform([ingredients_parsed])\n",
    "\n",
    "    # calculate cosine similarity between actual recipe ingreds and test ingreds\n",
    "    cos_sim = map(lambda x: cosine_similarity(ingredients_tfidf, x), tfidf_encodings)\n",
    "    scores = list(cos_sim)\n",
    "    #print(scores)\n",
    "\n",
    "    # Filter top N recommendations \n",
    "    recommendations = get_recommendations(N, scores)\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# Top-N recomendations order by score\n",
    "def get_recommendations(N, scores):\n",
    "    # load in recipe dataset \n",
    "    df_recipes = pd.read_csv(PARSED_PATH)\n",
    "    # order the scores with and filter to get the highest N scores\n",
    "    top = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:N]\n",
    "    # create dataframe to load in recommendations \n",
    "    # added \"dtype=\" to fix a pandas dataframe error\n",
    "    #recommendation = pd.DataFrame(columns = ['recipe', 'desc', 'ingredients', 'score', 'url'], dtype=object)\n",
    "    recommendation = pd.DataFrame(columns = ['recipe', 'desc', 'score', 'url'], dtype=object)\n",
    "    #print (recommendation)\n",
    "    count = 0\n",
    "    for i in top:\n",
    "        recommendation.at[count, 'recipe'] = title_parser(df_recipes['recipe_name'][i])\n",
    "        recommendation.at[count, 'desc'] = title_parser(df_recipes['desc'][i])\n",
    "        #recommendation.at[count, 'ingredients'] = ingredient_parser_final(df_recipes['ingredients'][i])\n",
    "        recommendation.at[count, 'url'] = df_recipes['recipe_urls'][i]\n",
    "        recommendation.at[count, 'score'] = \"{:.3f}\".format(float(scores[i])) #error here?\n",
    "        count += 1\n",
    "    return recommendation\n",
    "\n",
    "\n",
    "\n",
    "# neaten the ingredients being outputted \n",
    "def ingredient_parser_final(ingredient):\n",
    "    if isinstance(ingredient, list):\n",
    "        ingredients = ingredient\n",
    "    else:\n",
    "        ingredients = ast.literal_eval(ingredient)\n",
    "    \n",
    "    ingredients = ','.join(ingredients)\n",
    "    ingredients = unidecode.unidecode(ingredients)\n",
    "    return ingredients\n",
    "\n",
    "def title_parser(title):\n",
    "    title = unidecode.unidecode(title)\n",
    "    return title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parses the recipes into words\n",
    "recipe_df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "recipe_df[\"desc\"] = recipe_df['ingredients']\n",
    "# change the way the sentence is arranged in the data\n",
    "recipe_df['ingredients'] = recipe_df['ingredients'].map(str) + ',' + recipe_df['recipe_name'].map(str)\n",
    "recipe_df['ingredients'] = recipe_df['ingredients'].str.split()\n",
    "\n",
    "recipe_df['ingredients_parsed'] = recipe_df['ingredients'].apply(lambda x: recm.ingredient_parser(x))\n",
    "\n",
    "df = recipe_df[['recipe_name', 'desc', 'ingredients_parsed', 'ingredients', 'recipe_urls']]\n",
    "df = recipe_df.dropna()\n",
    "\n",
    "# remove - Allrecipes.com from end of every recipe title \n",
    "m = df.recipe_name.str.endswith('Recipe - Allrecipes.com')\n",
    "df['recipe_name'].loc[m] = df.recipe_name.loc[m].str[:-23]        \n",
    "df.to_csv(PARSED_PATH, index=False) #save the parsed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in parsed recipe dataset \n",
    "df_recipes = pd.read_csv(PARSED_PATH)\n",
    "df_recipes['ingredients_parsed'] = df_recipes.ingredients_parsed.values.astype('U')\n",
    "\n",
    "# TF-IDF feature extractor \n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(df_recipes['ingredients_parsed'])\n",
    "tfidf_recipe = tfidf.transform(df_recipes['ingredients_parsed'])\n",
    "\n",
    "# save the tfidf model and encodings \n",
    "with open(TFIDF_MODEL_PATH, \"wb\") as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "with open(TFIDF_ENCODING_PATH, \"wb\") as f:\n",
    "    pickle.dump(tfidf_recipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.752\n",
      "1    0.068\n",
      "2    0.062\n",
      "3    0.058\n",
      "4    0.058\n",
      "Name: score, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recipe</th>\n",
       "      <th>desc</th>\n",
       "      <th>score</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saree mahant fashion</td>\n",
       "      <td>[Banarasi Silk Sarees For Women Cotton Silk Wi...</td>\n",
       "      <td>0.752</td>\n",
       "      <td>29.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blouse generic</td>\n",
       "      <td>[BB Readymade 2by2 Blouse -Orange]</td>\n",
       "      <td>0.068</td>\n",
       "      <td>730.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blouse a.r.sundaram fashion</td>\n",
       "      <td>[Women s Brocade Blouse With All Asscesory of ...</td>\n",
       "      <td>0.062</td>\n",
       "      <td>638.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blouse gomore fashion</td>\n",
       "      <td>[Women s Embroidered Phantom Silk Blouse With ...</td>\n",
       "      <td>0.058</td>\n",
       "      <td>803.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blouse gomore fashion</td>\n",
       "      <td>[Women s Embroidered Phantom Silk Blouse With ...</td>\n",
       "      <td>0.058</td>\n",
       "      <td>826.jpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        recipe  \\\n",
       "0         saree mahant fashion   \n",
       "1               blouse generic   \n",
       "2  blouse a.r.sundaram fashion   \n",
       "3        blouse gomore fashion   \n",
       "4        blouse gomore fashion   \n",
       "\n",
       "                                                desc  score       url  \n",
       "0  [Banarasi Silk Sarees For Women Cotton Silk Wi...  0.752   29.jpeg  \n",
       "1                 [BB Readymade 2by2 Blouse -Orange]  0.068  730.jpeg  \n",
       "2  [Women s Brocade Blouse With All Asscesory of ...  0.062  638.jpeg  \n",
       "3  [Women s Embroidered Phantom Silk Blouse With ...  0.058  803.jpeg  \n",
       "4  [Women s Embroidered Phantom Silk Blouse With ...  0.058  826.jpeg  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the system\n",
    "test_ingredients = \"blouse mahant\"\n",
    "recs = RecSys(test_ingredients)\n",
    "print(recs.score)\n",
    "#print(recs)\n",
    "recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get version numbers of listed packages\n",
    "%pip list"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
