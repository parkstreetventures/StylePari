{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "177993a9",
   "metadata": {},
   "source": [
    "# Fashion Recommender\n",
    "\n",
    "Front End\n",
    "\n",
    "Sep 2021 - new and updated version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e3803e",
   "metadata": {},
   "source": [
    "The Flow\n",
    "\n",
    "- Front End\n",
    "- Calls API - Middle layer (Reads and processes data file )\n",
    "- Recommendation Algorithm\n",
    "- Color Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f4a5232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>desc</th>\n",
       "      <th>score</th>\n",
       "      <th>url</th>\n",
       "      <th>keywords</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saree maruti</td>\n",
       "      <td>Women's Linen Saree With Blouse Piece</td>\n",
       "      <td>0.550</td>\n",
       "      <td>./images/download_filename_1.jpg</td>\n",
       "      <td>linen blouse maruti</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saree maruti</td>\n",
       "      <td>Women's Linen Saree With Blouse Piece</td>\n",
       "      <td>0.573</td>\n",
       "      <td>./images/download_filename_1.jpg</td>\n",
       "      <td>linen blouse maruti</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          title                                   desc  score  \\\n",
       "0  saree maruti  Women's Linen Saree With Blouse Piece  0.550   \n",
       "0  saree maruti  Women's Linen Saree With Blouse Piece  0.573   \n",
       "\n",
       "                                url             keywords  color  \n",
       "0  ./images/download_filename_1.jpg  linen blouse maruti    red  \n",
       "0  ./images/download_filename_1.jpg  linen blouse maruti  black  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_choice = \"#00000\"\n",
    "fabric_choice = \"linen\"\n",
    "number_of_rec = 1\n",
    "\n",
    "recommendationEngine(color_choice, fabric_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4823cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendationEngine(color_choice, fabric_choice, N=1):\n",
    "    #color_name = find_name(color_choice)\n",
    "    color_name = \"red\"\n",
    "    parseDataFile()\n",
    "    createModel()\n",
    "    search_term = color_name + \" \" + fabric_choice\n",
    "    #getRecommendations(search_term, N=5)\n",
    "    df1 = getRecommendations(search_term, N)\n",
    "    df1['color'] = color_name\n",
    "    complementary = complementaryColor(color_name)\n",
    "    search_term = complementary + \" \" + fabric_choice\n",
    "    df2 = getRecommendations(search_term, N)\n",
    "    df2['color'] = complementary\n",
    "    frames = [df1, df2]\n",
    "    result = pd.concat(frames)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f53cc",
   "metadata": {},
   "source": [
    "# Build the intermediate layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "881d700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to all files\n",
    "DATA_PATH = \"./data/saree_data.csv\"\n",
    "CLEAN_PATH = \"./data/data_parsed_new.csv\"\n",
    "TFIDF_ENCODING_PATH = \"./model/data_tfidf_encodings.pkl\"\n",
    "TFIDF_MODEL_PATH = \"./model/data_tfidf.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f2e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EDA\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity,linear_kernel\n",
    "\n",
    "# other packages\n",
    "import nltk\n",
    "import string\n",
    "import ast\n",
    "import re\n",
    "import unidecode\n",
    "import unicodedata\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "import pickle \n",
    "import unidecode, ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe7dd8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "COLUMN_NAME = ['title', 'desc', 'keywords', 'url','score']\n",
    "\n",
    "# Load Our Dataset\n",
    "def loadData(fileName): \n",
    "    df = pd.read_csv(fileName)\n",
    "    return df \n",
    "\n",
    "#save the file\n",
    "def saveData(df, fileName): \n",
    "    df.to_csv(fileName, index=False)  \n",
    "    \n",
    "\n",
    "def parseDataFile():\n",
    "    \n",
    "    # parses the data into words\n",
    "    rec_df = loadData(DATA_PATH)\n",
    "    rec_df[COLUMN_NAME[1]] = rec_df['ingredients']\n",
    "    \n",
    "    # change the way the sentence is arranged in the data\n",
    "    rec_df['ingredients'] = rec_df['ingredients'].map(str) + ',' + rec_df['recipe_name'].map(str)\n",
    "    rec_df['ingredients'] = rec_df['ingredients'].str.split()\n",
    "\n",
    "    rec_df['ingredients_parsed'] = rec_df['ingredients'].apply(lambda x: ingredient_parser(x))\n",
    "\n",
    "    df = rec_df[['recipe_name', 'desc', 'ingredients_parsed', 'ingredients', 'recipe_urls']]\n",
    "    df = rec_df.dropna()\n",
    "    # delete the ingredients column\n",
    "    df.drop(['ingredients'], axis=1,inplace=True)\n",
    "    # rename all the columns \n",
    "    df.rename(columns={'recipe_name': COLUMN_NAME[0], 'ingredients_parsed': COLUMN_NAME[2], 'recipe_urls': COLUMN_NAME[3]}, inplace=True)\n",
    "\n",
    "    saveData(df,CLEAN_PATH)\n",
    "\n",
    "def createModel():\n",
    "   \n",
    "    # load in parsed recipe dataset     \n",
    "    df_rec = loadData(CLEAN_PATH)\n",
    "    df_rec[COLUMN_NAME[2]] = df_rec.desc.values.astype('U')\n",
    "\n",
    "    # TF-IDF feature extractor \n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf.fit(df_rec[COLUMN_NAME[2]])\n",
    "    tfidf_recipe = tfidf.transform(df_rec[COLUMN_NAME[2]])\n",
    "\n",
    "    # ------\n",
    "    #Printing the feature names\n",
    "    #print(tfidf.get_feature_names())\n",
    "    #matrix = tfidf_recipe.todense()\n",
    "    #tfidf_list = matrix.tolist()\n",
    "    #tfidf_df = pd.DataFrame(tfidf_list, columns = vectorizer.get_feature_names())\n",
    "    #print(tfidf_df)\n",
    "    # ------\n",
    "    \n",
    "    # save the tfidf model and encodings \n",
    "    with open(TFIDF_MODEL_PATH, \"wb\") as f:\n",
    "        pickle.dump(tfidf, f)\n",
    "\n",
    "    with open(TFIDF_ENCODING_PATH, \"wb\") as f:\n",
    "        pickle.dump(tfidf_recipe, f)\n",
    "\n",
    "\n",
    "def getRecommendations(ingredients, N):\n",
    "    \"\"\"\n",
    "    The reccomendation system takes in a list of ingredients and returns a list of top 5 \n",
    "    recipes based of of cosine similarity. \n",
    "    :param ingredients: a list of ingredients\n",
    "    :param N: the number of reccomendations returned \n",
    "    :return: top 5 reccomendations for cooking recipes\n",
    "    \"\"\"\n",
    "    # load in tdidf model and encodings \n",
    "    with open(TFIDF_ENCODING_PATH, 'rb') as f:\n",
    "        tfidf_encodings = pickle.load(f)\n",
    "\n",
    "    with open(TFIDF_MODEL_PATH, \"rb\") as f:\n",
    "        tfidf = pickle.load(f)\n",
    "\n",
    "    # parse the ingredients using the ingredient_parser \n",
    "    try: \n",
    "        ingredients_parsed = ingredient_parser(ingredients)\n",
    "    except:\n",
    "        ingredients_parsed = ingredient_parser([ingredients])\n",
    "    \n",
    "    # use our pretrained tfidf model to encode our input ingredients\n",
    "    ingredients_tfidf = tfidf.transform([ingredients_parsed])\n",
    "\n",
    "    # calculate cosine similarity between actual recipe ingreds and test ingreds\n",
    "    cos_sim = map(lambda x: cosine_similarity(ingredients_tfidf, x), tfidf_encodings)\n",
    "    scores = list(cos_sim)\n",
    "\n",
    "    # Filter top N recommendations \n",
    "    filtered_recommendations = filterRecommendations(N, scores)\n",
    "    return filtered_recommendations\n",
    "\n",
    "\n",
    "# Top-N recomendations order by score\n",
    "def filterRecommendations(N, scores):\n",
    "    # load in recipe dataset \n",
    "    df_rec = loadData(CLEAN_PATH)\n",
    "    # order the scores with and filter to get the highest N scores\n",
    "    top = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:N]\n",
    "    # create dataframe to load in recommendations \n",
    "    # added \"dtype=\" to fix a pandas dataframe error\n",
    "    #recommendation = pd.DataFrame(columns = ['recipe', 'desc', 'ingredients', 'score', 'url'], dtype=object)\n",
    "    recommendation = pd.DataFrame(columns = [COLUMN_NAME[0], COLUMN_NAME[1], COLUMN_NAME[4], COLUMN_NAME[3]], dtype=object)\n",
    "    #print (recommendation)\n",
    "    count = 0\n",
    "    for i in top:\n",
    "        #recommendation.at[count, 'url'] = df_rec['recipe_urls'][i]\n",
    "        recommendation.at[count, COLUMN_NAME[3]] = \"./images/download_filename_1.jpg\"\n",
    "        recommendation.at[count, COLUMN_NAME[0]] = title_parser(df_rec[COLUMN_NAME[0]][i])\n",
    "        recommendation.at[count, COLUMN_NAME[1]] = title_parser(df_rec[COLUMN_NAME[1]][i])\n",
    "        recommendation.at[count, COLUMN_NAME[2]] = df_rec[COLUMN_NAME[2]][i]\n",
    "        recommendation.at[count, COLUMN_NAME[4]] = \"{:.3f}\".format(float(scores[i])) #error here?\n",
    "        count += 1\n",
    "    return recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da450418",
   "metadata": {},
   "source": [
    "# this is the parser algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f598aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising stopwords for english\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# neaten the ingredients being outputted\n",
    "# this is not used anymore \n",
    "def ingredient_parser_final(ingredient):\n",
    "    \n",
    "    if isinstance(ingredient, list):\n",
    "        ingredients = ingredient\n",
    "    else:\n",
    "        ingredients = ast.literal_eval(ingredient)\n",
    "    \n",
    "    ingredients = ','.join(ingredients)\n",
    "    ingredients = unidecode.unidecode(ingredients)\n",
    "    return ingredients\n",
    "\n",
    "def title_parser(title):\n",
    "    title = unidecode.unidecode(title)\n",
    "    return title \n",
    "\n",
    "def ingredient_parser(ingreds):\n",
    "    \n",
    "    #showStatus(\"ingredient parser\")\n",
    "    words_to_remove = ['(',')','.','\\'','saree', 'matching', 'ba', 'gld', 'without', 'women', 'woman','shubh','self','fresh', 'trendz','oil', 'a', 'and',  'or',  'large', 'extra',  'free', 'small', 'from', 'higher', 'for', 'finely', 'freshly', 'to', 'organic', 'the', 'plain', 'plus' ]\n",
    "    # The ingredient list is now a string so we need to turn it back into a list. We use ast.literal_eval\n",
    "    if isinstance(ingreds, list):\n",
    "        ingredients = ingreds\n",
    "    else:\n",
    "        ingredients = ast.literal_eval(ingreds)\n",
    "    # We first get rid of all the punctuation. We make use of str.maketrans. It takes three input \n",
    "    # arguments 'x', 'y', 'z'. 'x' and 'y' must be equal-length strings and characters in 'x'\n",
    "    # are replaced by characters in 'y'. 'z' is a string (string.punctuation here) where each character\n",
    "    #  in the string is mapped to None. \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ingred_list = []\n",
    "    for i in ingredients:\n",
    "        i.translate(translator)\n",
    "        # We split up with hyphens as well as spaces\n",
    "        items = re.split(' |-', i)\n",
    "        # Get rid of words containing non alphabet letters\n",
    "        items = [word for word in items if word.isalpha()]\n",
    "        # Turn everything to lowercase\n",
    "        items = [word.lower() for word in items]\n",
    "      \n",
    "        # remove stop words\n",
    "        items = [word for word in items if word not in stop_words]\n",
    "        \n",
    "        # remove accents\n",
    "        items = [unidecode.unidecode(word) for word in items] #''.join((c for c in unicodedata.normalize('NFD', items) if unicodedata.category(c) != 'Mn'))\n",
    "        items = [unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore') for word in items]\n",
    "        \n",
    "        # Lemmatize words so we can compare words to measuring words\n",
    "        items = [lemmatizer.lemmatize(word) for word in items]\n",
    "        # Get rid of common easy words\n",
    "        items = [word for word in items if word not in words_to_remove]\n",
    "        # remove all square brackets\n",
    "        items = [remove_between_square_brackets(word) for word in items]\n",
    "        # remove all special characters\n",
    "        items = [remove_special_characters(word) for word in items]\n",
    "        if items:\n",
    "            ingred_list.append(' '.join(items)) \n",
    "    ingred_list = \" \".join(ingred_list)\n",
    "    return ingred_list\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5fe98f",
   "metadata": {},
   "source": [
    "# Other algorithms for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15d585f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x1132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7668 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "df_rec = loadData(CLEAN_PATH)\n",
    "news_articles_temp = df_rec.copy()\n",
    "headline_vectorizer = CountVectorizer()\n",
    "headline_features   = headline_vectorizer.fit_transform(news_articles_temp[COLUMN_NAME[2]])\n",
    "headline_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66c98982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Queried details ==============================\n",
      "headline :  saree trilok fab\n",
      "\n",
      " ========================= Recommended :  =======================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc</th>\n",
       "      <th>headline</th>\n",
       "      <th>Euclidean similarity with the queried article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Silk Saree with Blouse Piece</td>\n",
       "      <td>saree satrani</td>\n",
       "      <td>1.732051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Women's kanchipuram Silk Saree With Blouse Piece</td>\n",
       "      <td>saree varni fab</td>\n",
       "      <td>1.732051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               desc         headline  \\\n",
       "1                      Silk Saree with Blouse Piece    saree satrani   \n",
       "2  Women's kanchipuram Silk Saree With Blouse Piece  saree varni fab   \n",
       "\n",
       "   Euclidean similarity with the queried article  \n",
       "1                                       1.732051  \n",
       "2                                       1.732051  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bag of words model\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def bag_of_words_based_model(row_index, num_similar_items):\n",
    "    couple_dist = pairwise_distances(headline_features,headline_features[row_index])\n",
    "    indices = np.argsort(couple_dist.ravel())[0:num_similar_items]\n",
    "    df = pd.DataFrame({'desc':news_articles_temp['desc'][indices].values,\n",
    "               'headline':news_articles_temp['title'][indices].values,\n",
    "                'Euclidean similarity with the queried article': couple_dist[indices].ravel()})\n",
    "    print(\"=\"*30,\"Queried details\",\"=\"*30)\n",
    "    print('headline : ',news_articles_temp['title'][indices[0]])\n",
    "    print(\"\\n\",\"=\"*25,\"Recommended : \",\"=\"*23)\n",
    "    #print(df.head(5))\n",
    "    return df.iloc[1:,]\n",
    "    #return df.iloc[1:,]\n",
    "\n",
    "bag_of_words_based_model(1, 3) # Change the row index for any other queried article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c04431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TF-IDF method\n",
    "tfidf_headline_vectorizer = TfidfVectorizer(min_df = 0)\n",
    "tfidf_headline_features = tfidf_headline_vectorizer.fit_transform(news_articles_temp[COLUMN_NAME[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ebf6ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Queried article details ==============================\n",
      "headline :  saree trilok fab\n",
      "\n",
      " ========================= Recommended articles :  =======================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc</th>\n",
       "      <th>headline</th>\n",
       "      <th>Euclidean similarity with the queried article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traditional Readymade Saree Blouse RED</td>\n",
       "      <td>blouse fab 2 fashion</td>\n",
       "      <td>1.164623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     desc              headline  \\\n",
       "1  Traditional Readymade Saree Blouse RED  blouse fab 2 fashion   \n",
       "\n",
       "   Euclidean similarity with the queried article  \n",
       "1                                       1.164623  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tfidf_based_model(row_index, num_similar_items):\n",
    "    couple_dist = pairwise_distances(tfidf_headline_features,tfidf_headline_features[row_index])\n",
    "    indices = np.argsort(couple_dist.ravel())[0:num_similar_items]\n",
    "    df = pd.DataFrame({'desc':news_articles_temp['desc'][indices].values,\n",
    "               'headline':news_articles_temp['title'][indices].values,\n",
    "                'Euclidean similarity with the queried article': couple_dist[indices].ravel()})\n",
    "    print(\"=\"*30,\"Queried article details\",\"=\"*30)\n",
    "    print('headline : ',news_articles_temp['title'][indices[0]])\n",
    "    print(\"\\n\",\"=\"*25,\"Recommended articles : \",\"=\"*23)\n",
    "    \n",
    "    #return df.iloc[1:,1]\n",
    "    return df.iloc[1:,]\n",
    "tfidf_based_model(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f69f2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color functionality \n",
    "\n",
    "import re\n",
    "re_color = re.compile('#([0-9a-f]{2})([0-9a-f]{2})([0-9a-f]{2})')\n",
    "from math import sqrt\n",
    "\n",
    "def color_to_rgb(color):\n",
    "    return tuple(int(x, 16) / 255.0 for x in re_color.match(color).groups())\n",
    "\n",
    "def similarity(color1, color2):\n",
    "    \"\"\"Computes the pearson correlation coefficient for two colors. The result\n",
    "    will be between 1.0 (very similar) and -1.0 (no similarity).\"\"\"\n",
    "    c1 = color_to_rgb(color1)\n",
    "    c2 = color_to_rgb(color2)\n",
    "\n",
    "    s1 = sum(c1)\n",
    "    s2 = sum(c2)\n",
    "    sp1 = sum(map(lambda c: pow(c, 2), c1))\n",
    "    sp2 = sum(map(lambda c: pow(c, 2), c2))\n",
    "    sp = sum(map(lambda x: x[0] * x[1], zip(c1, c2)))\n",
    "\n",
    "    try:\n",
    "            computed = (sp - (s1 * s2 / 3.0)) / sqrt((sp1 - pow(s1, 2) / 3.0) * (sp2 - pow(s2, 2) / 3.0))\n",
    "    except:\n",
    "            computed = 0\n",
    "    \n",
    "    return computed\n",
    "\n",
    "color_names = {\n",
    "    '#000000': 'black',\n",
    "    '#ffffff': 'white',\n",
    "    '#808080': 'dark gray',\n",
    "    '#b0b0b0': 'light gray',\n",
    "    '#ff0000': 'red',\n",
    "    '#800000': 'dark red',\n",
    "    '#00ff00': 'green',\n",
    "    '#008000': 'dark green',\n",
    "    '#0000ff': 'blue',\n",
    "    '#000080': 'dark blue',\n",
    "    '#ffff00': 'yellow',\n",
    "    '#808000': 'olive',\n",
    "    '#00ffff': 'cyan',\n",
    "    '#ff00ff': 'magenta',\n",
    "    '#800080': 'purple'\n",
    "    }\n",
    "\n",
    "def find_name(color):\n",
    "    sim = [(similarity(color, c), name) for c, name in color_names.items()]\n",
    "    return max(sim, key=lambda x: x[0])[1]\n",
    "\n",
    "\n",
    "\n",
    "import random \n",
    "\n",
    "def complementaryColor(color_choice):\n",
    "    random_choice = ['red','blue','black','white','green','olive']\n",
    "    return random.choice(random_choice)\n",
    "\n",
    "# end color functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c837f49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
